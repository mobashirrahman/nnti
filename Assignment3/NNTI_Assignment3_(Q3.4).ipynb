{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W8Tlpr_sAI_"
   },
   "source": [
    "# NNTI Assignment 3\n",
    "\n",
    "Name 1: <br>\n",
    "Student id 1: <br>\n",
    "Email 1: <br>\n",
    "\n",
    "Name 2: <br>\n",
    "Student id 2:  <br>\n",
    "Email 2:  <br>\n",
    "\n",
    "Name 3: <br>\n",
    "Student id 3:  <br>\n",
    "Email 3: <br>\n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective  files for any additional exercises necessary. There is no need to resubmit the data files should they be provided separately. <br>\n",
    "\n",
    "\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_id1_Name2_id2_Name3_id3.zip **. Only one member of the group should make the submisssion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 792,
     "status": "ok",
     "timestamp": 1730621286572,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -330
    },
    "id": "TrO9uzx2rnkI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjhGbfPGvNCp"
   },
   "source": [
    "### Data setup: (Provided for you)\n",
    "We are using the MNIST dataset from [Pytorch](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html). For the sake of simplicity, we are only using only the first 2000 samples from the test set. Here, we have normalized the images to the range [0, 1], then reshaped each image to a tensor vector of size 784 and stack them together to get an array of 2000x784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1730622272923,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -330
    },
    "id": "uN0TwDP9s_Xu"
   },
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Extract the first 2000 samples\n",
    "data = mnist_data.data[:2000].float()\n",
    "labels = mnist_data.targets[:2000]\n",
    "\n",
    "# Normalize the data to [0, 1]\n",
    "data /= 255.0\n",
    "\n",
    "# Reshape the data\n",
    "n, d = data.size(0), data.size(1) * data.size(2)  # n = 2000, d = 784\n",
    "X = data.view(n, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B-YOCpZxK4Z"
   },
   "source": [
    "### a. Eigendecomposition [1 point]\n",
    "\n",
    "\n",
    "i.   Complete the function `eigen_decomp()`. Take care to:\n",
    "\n",
    "- Center the input data `X` by subtracting the mean of each feature.\n",
    "- Calculate the covariance matrix and performs eigendecomposition to obtain eigenvalues and eigenvectors.\n",
    "- Compute the cumulative variance explained by each component and return it.\n",
    "\n",
    "\n",
    "ii.   Plot the cumulative explained variance against the number of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730622274821,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -330
    },
    "id": "fbpZ1A_wtBzc"
   },
   "outputs": [],
   "source": [
    "def eigen_decomp(X):\n",
    "  \"\"\"\n",
    "    Performs eigendecomposition on the covariance matrix of the input data `X`.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): A 2D tensor where rows represent samples and columns represent features.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - eigvals (torch.Tensor): Eigenvalues of the covariance matrix, representing variance explained by each principal component.\n",
    "            - eigvecs (torch.Tensor): Eigenvectors of the covariance matrix, each representing a principal component direction.\n",
    "            - cumulative_variance_explained (np.ndarray): Cumulative variance explained by each principal component, as a numpy array.\n",
    "  \"\"\"\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "executionInfo": {
     "elapsed": 1844,
     "status": "ok",
     "timestamp": 1730622915191,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -330
    },
    "id": "XXzhFElYBijA",
    "outputId": "58a2b6ad-2baf-457c-8dbf-d681de6f768f"
   },
   "outputs": [],
   "source": [
    "eigvals, eigvecs, cumulative_variance_explained = eigen_decomp(X)\n",
    "\n",
    "# Plot the cumulative variance against the principal components\n",
    "##TODO##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WbUB9VKmM0Y"
   },
   "source": [
    "### Answer the below question: [0.25 points]\n",
    "\n",
    "Q: What do you think happens when we reduce the number of principal components $p$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DebvdRsXmfv3"
   },
   "source": [
    "Space for answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvREyJFYx3T3"
   },
   "source": [
    "### b. PCA [1 point]\n",
    "\n",
    "Complete the function `pca_reconstruction_error()`. It should:\n",
    "- Center the input data `X` by subtracting the mean of each feature (again).\n",
    "- Projects `X` onto the top `p` principal components, reconstructs it.\n",
    "- Compute the normalized reconstruction error in terms of the Frobenius norm, i.e. $e_{p} = \\frac{\\|x - \\hat{x}_p\\|_{F}}{\\|x\\|_{F}}$, where $x$ denotes the input matrix and $\\hat{x}_p$ denotes the recovered matrix associated to each $p$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1730622276562,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -330
    },
    "id": "QMZHr-6L95iK"
   },
   "outputs": [],
   "source": [
    "def pca_reconstruction_error(X, p, eigvecs):\n",
    "   \"\"\"\n",
    "    Calculates the PCA reconstruction error for the given data `X` using the top `p` principal components.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): A 2D tensor where rows represent samples and columns represent features.\n",
    "        p (int): The number of principal components to use for reconstruction.\n",
    "        eigvecs (torch.Tensor): Eigenvectors obtained from PCA, with each column representing a principal component direction.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - error (float): The normalized Frobenius norm of the difference between `X` and its PCA reconstruction, representing the reconstruction error.\n",
    "            - X_reconstructed (torch.Tensor): The reconstructed data using `p` principal components.\n",
    "   \"\"\"\n",
    "   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "executionInfo": {
     "elapsed": 2357,
     "status": "ok",
     "timestamp": 1730623025264,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -330
    },
    "id": "kjQQmudHC6TV",
    "outputId": "15146ea8-0983-4e7b-dbf9-33a12a3d77ab"
   },
   "outputs": [],
   "source": [
    "### Driver code for the previousky defined functions\n",
    "frobenius_norm = []\n",
    "\n",
    "# Evaluate for p = 50, 250, 500\n",
    "for p in [50, 250, 500]:\n",
    "    error, X_reconstructed = pca_reconstruction_error(X, p, eigvecs)\n",
    "    frobenius_norm.append(error)\n",
    "\n",
    "    # Visualize some original and reconstructed images\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "    for i in range(10):\n",
    "        axes[0, i].imshow(X[i].reshape(28, 28), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(X_reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "    plt.suptitle(f'Original vs Reconstructed Images (p={p})')\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Frobenius Norm Errors: {frobenius_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVYMTiWhUqiG"
   },
   "source": [
    "### c. Noisy Data Generation [1.5 points]\n",
    "\n",
    "i.   Complete the function `mnist_noised()` to generate noisy versions of the mnist data.\n",
    "\n",
    "ii.   Visualize the noisy images and compare them with their corresponding original images.\n",
    "\n",
    "iii.   Plot the cumulative explained variance of the **noisy data** along with the **original data** versus the number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "executionInfo": {
     "elapsed": 2309,
     "status": "ok",
     "timestamp": 1730623064176,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -330
    },
    "id": "gpl8Py-gtHA0",
    "outputId": "555c153a-288a-4704-b04f-75621cba5947"
   },
   "outputs": [],
   "source": [
    "# Add Gaussian noise\n",
    "def mnist_noised(X, noise_var=0.25):\n",
    "   \"\"\"\n",
    "    Adds Gaussian noise to the input data `X`.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): A 2D tensor where rows represent samples and columns represent features (e.g., flattened images).\n",
    "        noise_var (float, optional): The variance of the Gaussian noise to be added. Default is 0.25.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor with Gaussian noise added to each element of `X`..\n",
    "   \"\"\"\n",
    "   pass\n",
    "\n",
    "# Add Gaussian noise to the data\n",
    "noisy_data = mnist_noised(X)\n",
    "\n",
    "# Plot some original and noisy images\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i in range(10):\n",
    "    axes[0, i].imshow(X[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(noisy_data[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "plt.suptitle('Original vs Noisy Images')\n",
    "plt.show()\n",
    "\n",
    "# Compute eigen decomposition for the noisy data\n",
    "eigvals_noisy, eigvecs_noisy, cumulative_variance_explained_noisy = eigen_decomp(noisy_data)\n",
    "\n",
    "# Plot the cumulative variance for noisy data and original data\n",
    "##TODO##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdP4UWybm5X2"
   },
   "source": [
    "### Answer the below question: [0.25 points]\n",
    "\n",
    "Q: How do the 2 plots compare? Are they similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU-0UI5bm5X3"
   },
   "source": [
    "Space for answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBMjUWJ1Vfmq"
   },
   "source": [
    "### d. Image Denoising [1 point]\n",
    "\n",
    "\n",
    "\n",
    "i.   Complete the function `pca_reconstruction_noisy()`. It should:\n",
    "- Add Gaussian noise of a specified variance ($\\sigma^{2}$) to the original data.\n",
    "- Compute the eigen decomposition on the noisy data, retaining only the top `p` principal components.\n",
    "- Calculate the reconstruction error to quantify the quality of denoising.\n",
    "\n",
    "ii. Plot the denoised images agianst the noisy data for visual comparison.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1730625054778,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -330
    },
    "id": "FJqz4fhUSpJq"
   },
   "outputs": [],
   "source": [
    "def pca_reconstruction_noisy(X, noise_var=0.05, p=100):\n",
    "   \"\"\"\n",
    "    Adds Gaussian noise to input data `X`, performs PCA-based dimensionality reduction,\n",
    "    and reconstructs the denoised images using the top `p` principal components.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): A 2D tensor where rows represent samples and columns represent features (e.g., flattened images).\n",
    "        noise_var (float, optional): Variance of the Gaussian noise to be added to `X`. Default is 0.05.\n",
    "        p (int, optional): Number of principal components to retain for PCA-based reconstruction. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - error (float): The normalized reconstruction error after applying PCA to the noisy data.\n",
    "            - X_reconstructed (torch.Tensor): A 2D tensor of reconstructed data using the top `p` components, representing the denoised images.\n",
    "   \"\"\"\n",
    "   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5530,
     "status": "ok",
     "timestamp": 1730625064653,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -330
    },
    "id": "DK1xiyNNNYAM",
    "outputId": "215fd759-f57e-4fb0-98e1-23bb74b3e002"
   },
   "outputs": [],
   "source": [
    "### Driver code for the previousky defined functions\n",
    "noise_levels = [0.05, 0.15, 0.25, 0.5]\n",
    "errors = []\n",
    "\n",
    "for noise_var in noise_levels:\n",
    "    error, X_reconstructed = pca_reconstruction_noisy(X, noise_var, p=200)\n",
    "    errors.append(error)\n",
    "\n",
    "    # Visualize some of the recovered images\n",
    "    ##TODO##\n",
    "\n",
    "print(f\"Reconstruction Errors for different noise levels: {errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_BR0Al-mJ87"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhR4Y7sv4uuNe3pS1Id/el",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
