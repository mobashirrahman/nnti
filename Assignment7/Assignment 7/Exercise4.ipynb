{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f0490b-717f-46e4-81a5-1e499de95742",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "In this exercise, you will implement forward and backward pass of a simple neural network. You are expected to write all the functions using vectorized numpy operations only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc1f734-9e54-4204-b367-b2cbbd4eee37",
   "metadata": {},
   "source": [
    "The following cell has code to load the train and test data. You will be working with the MNIST dataset. The images have been flattened and normalised to be between 0 and 1 for you already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323945ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load MNIST dataset\n",
    "def load_mnist():\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    X, y = mnist.data / 255.0, mnist.target.astype(int)\n",
    "    return X, y.to_numpy()  # Convert y to a NumPy array\n",
    "\n",
    "# One-hot encode labels\n",
    "def one_hot_encode(y, num_classes):\n",
    "    encoder = OneHotEncoder(sparse_output=False, categories=[range(num_classes)])\n",
    "    return encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split dataset\n",
    "def prepare_data(test_size=0.2):\n",
    "    X, y = load_mnist()\n",
    "    y_encoded = one_hot_encode(y, num_classes=10)\n",
    "    return train_test_split(X, y_encoded, test_size=test_size, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepare_data()\n",
    "X_train, X_test = X_train.to_numpy(), X_test.to_numpy()\n",
    "print(f\"Training Data Shape: {X_train.shape}, Test Data Shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9156fa0-70dc-4330-a267-47cc0bf8f399",
   "metadata": {},
   "source": [
    "You need to implement a two-layer neural network (one hidden layer) using NumPy. Fill in all the required cells below. Only use numpy functions.\n",
    "\n",
    "- Implement the forward pass. Use ReLU activation for the hidden layer and softmax for the final output. Be sure to use the bias as well. (0.5 point)\n",
    "- Implement the backward pass. This should return the gradients of the loss w.r.t the weights and biases of the network. The return signature of the backward pass is provided as a comment in the function. (1.5 points)\n",
    "- For your loss function, use the cross-entropy loss. (0.5 point)\n",
    "- The `predict` function should run the forward pass and return the predicted class. (0.5 point)\n",
    "- The `train` function should run the forward pass, compute the loss and and the gradients, and update the parameters using gradient descent with the given learning rate. It should repeat this for the given number of epochs. You are given some code to evaluate the performance of your network during training. You can uncomment it and match your variable names. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39532323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases.\n",
    "        \"\"\"\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def relu(self, Z):\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def relu_derivative(self, Z):\n",
    "        \"\"\"\n",
    "        Derivative of ReLU activation.\n",
    "        \"\"\"\n",
    "        return (Z > 0).astype(float)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"\n",
    "        Softmax activation function.\n",
    "        \"\"\"\n",
    "        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        # TO DO: Implement forward pass\n",
    "        pass\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        \"\"\"\n",
    "        Backpropagation to update weights.\n",
    "        \"\"\"\n",
    "        # TO DO: Implement backward pass\n",
    "        # return dW1, db1, dW2, db2\n",
    "        pass\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss.\n",
    "        \"\"\"\n",
    "        # TO DO: Implement cross-entropy loss\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities and labels.\n",
    "        \"\"\"\n",
    "        # TO DO: Implement prediction logic\n",
    "        pass\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        \"\"\"\n",
    "        Train the model using gradient descent.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # TO DO: Implement training loop\n",
    "            # if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            #     print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4df5d7-1f43-4a1c-88a5-1e970816dec7",
   "metadata": {},
   "source": [
    "The following code evaluates the performance of your network on X_test. You can expect an accuracy of around 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d066af0c-dce7-41d6-b928-104e2691a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64  # You can choose a suitable value\n",
    "output_size = 10  # Number of classes\n",
    "\n",
    "model = TwoLayerNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Training the model\n",
    "epochs = 100\n",
    "learning_rate = 0.5\n",
    "model.train(X_train, y_train, epochs, learning_rate)\n",
    "\n",
    "# Evaluate on test data\n",
    "predictions = model.predict(X_test)\n",
    "# print(predictions.shape)\n",
    "accuracy = np.mean(predictions == np.argmax(y_test, axis=1))\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
