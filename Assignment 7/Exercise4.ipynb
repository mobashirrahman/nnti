{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f0490b-717f-46e4-81a5-1e499de95742",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "In this exercise, you will implement forward and backward pass of a simple neural network. You are expected to write all the functions using vectorized numpy operations only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc1f734-9e54-4204-b367-b2cbbd4eee37",
   "metadata": {},
   "source": [
    "The following cell has code to load the train and test data. You will be working with the MNIST dataset. The images have been flattened and normalised to be between 0 and 1 for you already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323945ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (56000, 784), Test Data Shape: (14000, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load MNIST dataset\n",
    "def load_mnist():\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    X, y = mnist.data / 255.0, mnist.target.astype(int)\n",
    "    return X, y.to_numpy()  # Convert y to a NumPy array\n",
    "\n",
    "# One-hot encode labels\n",
    "def one_hot_encode(y, num_classes):\n",
    "    encoder = OneHotEncoder(sparse_output=False, categories=[range(num_classes)])\n",
    "    return encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split dataset\n",
    "def prepare_data(test_size=0.2):\n",
    "    X, y = load_mnist()\n",
    "    y_encoded = one_hot_encode(y, num_classes=10)\n",
    "    return train_test_split(X, y_encoded, test_size=test_size, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepare_data()\n",
    "X_train, X_test = X_train.to_numpy(), X_test.to_numpy()\n",
    "print(f\"Training Data Shape: {X_train.shape}, Test Data Shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9156fa0-70dc-4330-a267-47cc0bf8f399",
   "metadata": {},
   "source": [
    "You need to implement a two-layer neural network (one hidden layer) using NumPy. Fill in all the required cells below. Only use numpy functions.\n",
    "\n",
    "- Implement the forward pass. Use ReLU activation for the hidden layer and softmax for the final output. Be sure to use the bias as well. (0.5 point)\n",
    "- Implement the backward pass. This should return the gradients of the loss w.r.t the weights and biases of the network. The return signature of the backward pass is provided as a comment in the function. (1.5 points)\n",
    "- For your loss function, use the cross-entropy loss. (0.5 point)\n",
    "- The `predict` function should run the forward pass and return the predicted class. (0.5 point)\n",
    "- The `train` function should run the forward pass, compute the loss and and the gradients, and update the parameters using gradient descent with the given learning rate. It should repeat this for the given number of epochs. You are given some code to evaluate the performance of your network during training. You can uncomment it and match your variable names. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39532323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases.\n",
    "        \"\"\"\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def relu(self, Z):\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def relu_derivative(self, Z):\n",
    "        \"\"\"\n",
    "        Derivative of ReLU activation.\n",
    "        \"\"\"\n",
    "        return (Z > 0).astype(float)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"\n",
    "        Softmax activation function.\n",
    "        \"\"\"\n",
    "        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        # Compute Z1 and A1\n",
    "        self.Z1 = X.dot(self.W1) + self.b1\n",
    "        self.A1 = self.relu(self.Z1)\n",
    "\n",
    "        # Compute Z2 and A2 (softmax output)\n",
    "        self.Z2 = self.A1.dot(self.W2) + self.b2\n",
    "        self.A2 = self.softmax(self.Z2)\n",
    "        return self.A2\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss.\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        # Avoid log(0)\n",
    "        log_likelihood = -np.log(y_pred[np.arange(m), np.argmax(y_true, axis=1)])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        \"\"\"\n",
    "        Backpropagation to update weights.\n",
    "        \n",
    "        Returns:\n",
    "            dW1, db1, dW2, db2\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # dZ2 = A2 - y\n",
    "        dZ2 = self.A2 - y\n",
    "\n",
    "        # dW2 = A1^T dZ2\n",
    "        dW2 = (self.A1.T).dot(dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "        # dA1 = dZ2 W2^T\n",
    "        dA1 = dZ2.dot(self.W2.T)\n",
    "\n",
    "        # dZ1 = dA1 * relu'(Z1)\n",
    "        dZ1 = dA1 * self.relu_derivative(self.Z1)\n",
    "\n",
    "        # dW1 = X^T dZ1\n",
    "        dW1 = X.T.dot(dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Update parameters\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \"\"\"\n",
    "        A2 = self.forward(X)\n",
    "        return np.argmax(A2, axis=1)\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "\n",
    "            # Backward pass (and update parameters)\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "            # Print loss every 10 epochs or on the last epoch\n",
    "            if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4df5d7-1f43-4a1c-88a5-1e970816dec7",
   "metadata": {},
   "source": [
    "The following code evaluates the performance of your network on X_test. You can expect an accuracy of around 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d066af0c-dce7-41d6-b928-104e2691a5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.3008\n",
      "Epoch 11/100, Loss: 1.7980\n",
      "Epoch 21/100, Loss: 0.8264\n",
      "Epoch 31/100, Loss: 0.8380\n",
      "Epoch 41/100, Loss: 0.5112\n",
      "Epoch 51/100, Loss: 0.4713\n",
      "Epoch 61/100, Loss: 0.4140\n",
      "Epoch 71/100, Loss: 0.3930\n",
      "Epoch 81/100, Loss: 0.3480\n",
      "Epoch 91/100, Loss: 0.3319\n",
      "Epoch 100/100, Loss: 0.3218\n",
      "Test Accuracy: 90.88%\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 256  # You can choose a suitable value\n",
    "output_size = 10  # Number of classes\n",
    "\n",
    "model = TwoLayerNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Training the model\n",
    "epochs = 100\n",
    "learning_rate = 0.5\n",
    "model.train(X_train, y_train, epochs, learning_rate)\n",
    "\n",
    "# Evaluate on test data\n",
    "predictions = model.predict(X_test)\n",
    "# print(predictions.shape)\n",
    "accuracy = np.mean(predictions == np.argmax(y_test, axis=1))\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
